{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "steering_optimizer_colab",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamasdoka/str_opt/blob/dev/steering_optimizer_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2",
        "colab_type": "text"
      },
      "source": [
        "# Steering geometry optimizer with stable baselines\n",
        "\n",
        "Github Repo: [https://github.com/tamasdoka/str_opt](https://github.com/tamasdoka/str_opt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "#!pip install stable-baselines[mpi]==2.8.0 box2d box2d-kengz\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm",
        "colab_type": "text"
      },
      "source": [
        "# Import gym, numpy and stable baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines import DQN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd",
        "colab_type": "text"
      },
      "source": [
        "## Cloning and intalling the Gym env and instantiate the agent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJIBrY1bVorI",
        "colab_type": "code",
        "outputId": "7c51cf9f-4b97-4f63-d7d5-bb7183975051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone -b dev https://github.com/tamasdoka/str_opt/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'str_opt'...\n",
            "remote: Enumerating objects: 354, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (212/212), done.\u001b[K\n",
            "remote: Total 354 (delta 152), reused 297 (delta 131), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (354/354), 1.05 MiB | 1.42 MiB/s, done.\n",
            "Resolving deltas: 100% (152/152), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgvi0izGPnhj",
        "colab_type": "code",
        "outputId": "bd4d71e4-ff9a-409b-d924-4cc9fe02c35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install -e /content/str_opt/steering-optimizer"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/str_opt/steering-optimizer\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from steering-optimizer==0.0.1) (0.15.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->steering-optimizer==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->steering-optimizer==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym->steering-optimizer==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->steering-optimizer==0.0.1) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->steering-optimizer==0.0.1) (1.17.4)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->steering-optimizer==0.0.1) (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym->steering-optimizer==0.0.1) (0.16.0)\n",
            "Installing collected packages: steering-optimizer\n",
            "  Running setup.py develop for steering-optimizer\n",
            "Successfully installed steering-optimizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZN-i2LNSifp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for env in gym.envs.registry.env_specs.keys():\n",
        "     if 'steering_optimizer' in env:\n",
        "          print('Remove {} from registry\".format(env)')\n",
        "          del gym.envs.registry.env_specs[env]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Rg6-kgSni2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    id='steering_optimizer-v0',\n",
        "    entry_point='steering_optimizer.envs:StrOptEnv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2XspjGzopA3",
        "colab_type": "code",
        "outputId": "f5d55c6d-35ee-46d4-b73c-628086e28f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/str_opt/steering-optimizer/\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/str_opt/steering-optimizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn8YMfaKX7QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('steering_optimizer-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiskmFbm4x01",
        "colab_type": "code",
        "outputId": "640d95d3-2520-4ba7-c730-226d9d641f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env.check_version()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "StrOpt version: dev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOsgjwreAusW",
        "colab_type": "text"
      },
      "source": [
        "## Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DQN('MlpPolicy', env, learning_rate=1e-3, exploration_fraction=0.5, prioritized_replay=True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl",
        "colab_type": "text"
      },
      "source": [
        "Function for model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63M8mSKR-6Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, num_steps=100):\n",
        "  \"\"\"\n",
        "  Evaluate a RL agent\n",
        "  :param model: (BaseRLModel object) the RL Agent\n",
        "  :param num_steps: (int) number of timesteps to evaluate it\n",
        "  :return: (float) Mean reward for the last 100 episodes\n",
        "  \"\"\"\n",
        "  episode_rewards = [0.0]\n",
        "  rewards = []\n",
        "  states = []\n",
        "  obs = env.reset()\n",
        "  print('state:', obs)\n",
        "  for i in range(num_steps):\n",
        "      # _states are only useful when using LSTM policies\n",
        "      action, _states = model.predict(obs)\n",
        "\n",
        "      #print('action:', action)\n",
        "\n",
        "      obs, reward, done, info = env.step(action)\n",
        "\n",
        "      #print('obs (state), reward, done:', obs, reward, done)\n",
        "      \n",
        "      # Stats\n",
        "      episode_rewards[-1] += reward\n",
        "      rewards.append(reward)\n",
        "      states.append(obs)\n",
        "      if done:\n",
        "          obs = env.reset()\n",
        "          episode_rewards.append(0.0)\n",
        "  # Compute mean reward for the last 100 episodes\n",
        "  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "  print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "  best_reward_index = np.argmax(episode_rewards)\n",
        "  best_state = states[best_reward_index]\n",
        "  best_reward = rewards[best_reward_index]\n",
        "\n",
        "  print('Best reward:', best_reward)\n",
        "  print('Best state:', best_state)\n",
        "  \n",
        "  return mean_100ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK",
        "colab_type": "text"
      },
      "source": [
        "Model before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHLMA6NFk95",
        "colab_type": "code",
        "outputId": "7b0dd878-9252-40e3-94f8-4c117a0077b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_steps=1000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state: [ -95.62267323 -182.59717912 -419.34229977  -97.08271741]\n",
            "Mean reward: 0.1 Num episodes: 133\n",
            "Best reward: 0.01\n",
            "Best state: [ -93.67301898 -171.07768053 -422.78257589  -86.18850968]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE",
        "colab_type": "text"
      },
      "source": [
        "## Train the agent and save it\n",
        "\n",
        "Warning: this may take a while"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cfSXIB-pTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the agent\n",
        "model.learn(total_timesteps=int(2e4), log_interval=10)\n",
        "# Save the agent\n",
        "model.save(\"dqn_steering_opt\")\n",
        "del model  # delete trained model to demonstrate loading"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T31dZJYNrJwF",
        "colab_type": "text"
      },
      "source": [
        "## Load the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ExgtyZrIA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8b32f93-3b68-4a87-d2e3-bfcb186b09ab"
      },
      "source": [
        "model = DQN.load(\"dqn_steering_opt\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygl_gVmV_QP7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e02ee11f-66e5-4944-e124-2ba066138430"
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward = evaluate(model, num_steps=5000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state: [-116.80394909 -203.22429275 -418.18714272 -100.74071472]\n",
            "Mean reward: -0.2 Num episodes: 1119\n",
            "Best reward: 0.01\n",
            "Best state: [-104.18619417 -196.96378768 -417.48413011 -122.96692131]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQDZI5VEGnUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}