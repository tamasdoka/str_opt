{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/asus_strix_scar/Anaconda3/Lib/site-packages/gym/envs/steering-optimizer/steering_optimizer/envs/github/steering-optimizer\n",
      "Requirement already satisfied: gym in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from steering-optimizer==0.0.1) (0.15.4)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: six in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.16.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (4.1.2.30)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: future in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym->steering-optimizer==0.0.1) (0.17.1)\n",
      "Installing collected packages: steering-optimizer\n",
      "  Found existing installation: steering-optimizer 0.0.1\n",
      "    Uninstalling steering-optimizer-0.0.1:\n",
      "      Successfully uninstalled steering-optimizer-0.0.1\n",
      "  Running setup.py develop for steering-optimizer\n",
      "Successfully installed steering-optimizer\n"
     ]
    }
   ],
   "source": [
    "!pip install -e C:\\Users\\asus_strix_scar\\Anaconda3\\Lib\\site-packages\\gym\\envs\\steering-optimizer\\steering_optimizer\\envs\\github\\steering-optimizer\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.env_specs.keys():\n",
    "     if 'steering_optimizer' in env:\n",
    "          print('Remove {} from registry\".format(env)')\n",
    "          del gym.envs.registry.env_specs[env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='steering_optimizer-v0',\n",
    "    entry_point='steering_optimizer.envs:StrOptEnv',# kwargs={'WB': 1900, 'TW': 1200, 'KP': 150,'tr_min': 4000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"steering_optimizer-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed parameters\n",
    "\n",
    "#Wheelbase, track width and kingpin distance\n",
    "\n",
    "WB = 1900\n",
    "TW = 1200\n",
    "KP = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "session_size = 150\n",
    "t_max = 300\n",
    "percentile = 80\n",
    "hidden_size = 15\n",
    "learning_rate = 0.01\n",
    "completion_score = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        \n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc0(x))\n",
    "        return self.fc1(x)\n",
    "\n",
    "    def generate_batch(self, env, batch_size, t_max):\n",
    "        \n",
    "        activation_1 = nn.Softmax(dim=1)\n",
    "        batch_actions, batch_states, batch_rewards = [],[],[]\n",
    "        # Reset only once in a batch\n",
    "        s_0 = env.reset()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            states,actions = [],[]\n",
    "            total_reward = 0\n",
    "            \n",
    "            #s = env.reset()\n",
    "            # Imitating reset, with the same state for every batch member\n",
    "            env.state = s_0\n",
    "            \n",
    "            s = s_0\n",
    "            \n",
    "            env.steps_beyond_done = None\n",
    "            env.error = None\n",
    "            env.max_r = None\n",
    "            env.steps_since_reset = 0\n",
    "            env.total_reward = 0\n",
    "            \n",
    "            #print('state shape after reset:', s.shape)\n",
    "            s = s.reshape((1,4))\n",
    "            \n",
    "            #print('state shape after reshape:', s.shape)\n",
    "            \n",
    "            # step counter\n",
    "            count = 0\n",
    "            \n",
    "            for t in range(t_max):\n",
    "                count +=1\n",
    "                \n",
    "                # Scaling state to be within -1 and 1\n",
    "                s = s/TW*2\n",
    "                \n",
    "                s_v = torch.FloatTensor([s]).reshape(((1,4)))\n",
    "                #print('state_shape',s_v.shape)\n",
    "                #s_v = torch.FloatTensor([s])\n",
    "                \n",
    "                #print('state: x',s_v)\n",
    "                act_probs_v = activation_1(self.forward(s_v))\n",
    "                act_probs = act_probs_v.data.numpy()[0]\n",
    "                choice_num = len(act_probs)\n",
    "                #print('num:',choice_num)\n",
    "                a = np.random.choice(choice_num, p=act_probs)\n",
    "                \n",
    "\n",
    "                new_s, r, done, info = env.step(a)\n",
    "                \n",
    "                \n",
    "\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                \n",
    "                #print('new state',new_s)\n",
    "                \n",
    "                total_reward += r\n",
    "                \n",
    "                #print('action:',a, 'reward: ', r, 'error:', env.error, 'total reward:', total_reward)\n",
    "                \n",
    "                # print('total reward:', total_reward)\n",
    "                # Getting the new state               \n",
    "                s = new_s/TW*2\n",
    "                \n",
    "                # After done, no more steps\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            batch_actions.append(actions)\n",
    "            #print('Batch member done')\n",
    "            #print('Batch actions:', batch_actions[-1])\n",
    "            #print('total reward on episode:', total_reward)\n",
    "            \n",
    "            batch_states.append(states)\n",
    "            batch_rewards.append(total_reward)\n",
    "        \n",
    "        #print('Batch generated')\n",
    "        #print('States:', batch_states)\n",
    "        #print('Episode Actions:', batch_actions)\n",
    "        #print('Rewards:', batch_rewards)\n",
    "        \n",
    "        return batch_states, batch_actions, batch_rewards\n",
    "\n",
    "    def filter_batch(self, states_batch, actions_batch, rewards_batch, percentile):\n",
    "        \n",
    "        reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "        \n",
    "        #print('reward threshold:', reward_threshold)\n",
    "        \n",
    "        threshold_cnt = 0\n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                threshold_cnt += 1\n",
    "                \n",
    "        elite_states = np.array([], dtype=float)\n",
    "        elite_actions = np.array([], dtype=int)\n",
    "        \n",
    "        elite_indices = []\n",
    "        \n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                elite_indices.append(i)\n",
    "        \n",
    "        elite_indices = np.asarray(elite_indices)\n",
    "        #print('indices', elite_indices)\n",
    "        \n",
    "        obs_num = np.shape(env.observation_space)[0]\n",
    "        #print('obs num:', obs_num)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(len(states_batch[i])):\n",
    "                    \n",
    "                if j == 0:\n",
    "                    elite_states = np.append(elite_states, states_batch[i][0][0]) \n",
    "                    elite_actions = np.append(elite_actions, actions_batch[0][0])\n",
    "\n",
    "                else:\n",
    "                    elite_states = np.append(elite_states, states_batch[i][j])\n",
    "                    elite_actions = np.append(elite_actions, actions_batch[i][j])\n",
    "\n",
    "        return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = np.shape(env.observation_space)[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#print(n_states, n_actions)\n",
    "\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.08580, reward_mean=0.23436, reward_threshold=0.34350\n",
      "1: loss=1.08238, reward_mean=0.29088, reward_threshold=0.40490\n",
      "2: loss=1.07683, reward_mean=0.32390, reward_threshold=0.49200\n",
      "3: loss=1.07255, reward_mean=0.38574, reward_threshold=0.49210\n",
      "4: loss=1.07077, reward_mean=0.39582, reward_threshold=0.47930\n",
      "5: loss=1.06514, reward_mean=0.48648, reward_threshold=0.58150\n",
      "6: loss=1.05741, reward_mean=0.52194, reward_threshold=0.61600\n",
      "7: loss=1.05715, reward_mean=0.54054, reward_threshold=0.63810\n",
      "8: loss=1.05186, reward_mean=0.56766, reward_threshold=0.69220\n"
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "stats1 = np.array([])\n",
    "stats2 = np.array([])\n",
    "stats3 = np.array([])\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = net.generate_batch(env, batch_size, t_max)\n",
    "    \n",
    "    #print('batch_states', np.asarray(batch_states).reshape(-1,4))\n",
    "\n",
    "    elite_states, elite_actions = net.filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    #print('el_states', elite_states)\n",
    "    elite_states = elite_states.reshape(-1,4)\n",
    "    \n",
    "    #print('el_states: reshaped', elite_states)\n",
    "    #print('el_actions', elite_actions)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "\n",
    "    action_scores_v = net(tensor_states)\n",
    "    \n",
    "    #print(action_scores_v)\n",
    "    \n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.5f, reward_mean=%.5f, reward_threshold=%.5f\"% (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    stats1 = np.append(stats1, loss_v.item())\n",
    "    stats2 = np.append(stats2, mean_reward)\n",
    "    stats3 = np.append(stats3, threshold)\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stats1)\n",
    "plt.figure()\n",
    "plt.plot(stats2)\n",
    "plt.figure()\n",
    "plt.plot(stats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
