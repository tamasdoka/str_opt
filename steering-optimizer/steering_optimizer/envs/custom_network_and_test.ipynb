{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/asus_strix_scar/Anaconda3/Lib/site-packages/gym/envs/steering-optimizer/steering_optimizer/envs/github/steering-optimizer\n",
      "Requirement already satisfied: gym in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from steering-optimizer==0.0.1) (0.15.4)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.16.5)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (4.1.2.30)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: future in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym->steering-optimizer==0.0.1) (0.17.1)\n",
      "Installing collected packages: steering-optimizer\n",
      "  Found existing installation: steering-optimizer 0.0.1\n",
      "    Uninstalling steering-optimizer-0.0.1:\n",
      "      Successfully uninstalled steering-optimizer-0.0.1\n",
      "  Running setup.py develop for steering-optimizer\n",
      "Successfully installed steering-optimizer\n"
     ]
    }
   ],
   "source": [
    "!pip install -e C:\\Users\\asus_strix_scar\\Anaconda3\\Lib\\site-packages\\gym\\envs\\steering-optimizer\\steering_optimizer\\envs\\github\\steering-optimizer\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.env_specs.keys():\n",
    "     if 'steering_optimizer' in env:\n",
    "          print('Remove {} from registry\".format(env)')\n",
    "          del gym.envs.registry.env_specs[env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='steering_optimizer-v0',\n",
    "    entry_point='steering_optimizer.envs:StrOptEnv',# kwargs={'WB': 1900, 'TW': 1200, 'KP': 150,'tr_min': 4000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"steering_optimizer-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed parameters\n",
    "\n",
    "#Wheelbase, track width and kingpin distance\n",
    "\n",
    "WB = 1900\n",
    "TW = 1200\n",
    "KP = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "session_size = 100\n",
    "t_max = 200\n",
    "percentile = 80\n",
    "hidden_size = 30\n",
    "learning_rate = 0.005\n",
    "completion_score = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        \n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc0(x))\n",
    "        return self.fc1(x)\n",
    "\n",
    "    def generate_batch(self, env, batch_size, t_max):\n",
    "        \n",
    "        activation_1 = nn.Softmax(dim=1)\n",
    "        batch_actions, batch_states, batch_rewards = [],[],[]\n",
    "        # Reset only once in a batch\n",
    "        s_0 = env.reset()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            states,actions = [],[]\n",
    "            total_reward = 0\n",
    "            \n",
    "            #s = env.reset()\n",
    "            \n",
    "            # Imitating reset, with the same state for every batch member\n",
    "            \n",
    "            env.state = s_0\n",
    "            \n",
    "            s = s_0\n",
    "            \n",
    "            env.steps_beyond_done = None\n",
    "            env.error = None\n",
    "            env.max_r = None\n",
    "            env.steps_since_reset = 0\n",
    "            env.total_reward = 0\n",
    "            \n",
    "            #print('state shape after reset:', s.shape)\n",
    "            s = s.reshape((1,4))\n",
    "            \n",
    "             # Scaling the state width TW\n",
    "            s = s #/TW*2\n",
    "            #print('state shape after reshape:', s.shape)\n",
    "            \n",
    "            count = 0\n",
    "            #print('batch:',b)\n",
    "            \n",
    "            for t in range(t_max):\n",
    "                count +=1\n",
    "                \n",
    "                s_v = torch.FloatTensor([s]).reshape(((1,4)))\n",
    "                #print('state_shape',s_v.shape)\n",
    "                #s_v = torch.FloatTensor([s])\n",
    "                \n",
    "                #print('state: x',s_v)\n",
    "                \n",
    "                act_probs_v = activation_1(self.forward(s_v))\n",
    "                act_probs = act_probs_v.data.numpy()[0]\n",
    "                choice_num = len(act_probs)\n",
    "                #print('num:',choice_num)\n",
    "                a = np.random.choice(choice_num, p=act_probs)\n",
    "                #print('action:',a)\n",
    "\n",
    "                new_s, r, done, info = env.step(a)\n",
    "\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                \n",
    "                #print('new state',new_s)\n",
    "                \n",
    "                total_reward += r\n",
    "                \n",
    "                #print('total reward on episode:', total_reward)\n",
    "                 # Scaling the state width TW               \n",
    "                s = new_s #/TW*2\n",
    "            \n",
    "                \n",
    "            #print('Batch member done')\n",
    "            batch_actions.append(actions)\n",
    "            #print('Batch actions:', batch_actions[-1])\n",
    "            #print('total reward on episode:', total_reward)\n",
    "            \n",
    "            batch_states.append(states)\n",
    "            batch_rewards.append(total_reward)\n",
    "        \n",
    "        #print('Batch generated')\n",
    "        #print('States:', batch_states)\n",
    "        #print('Episode Actions:', batch_actions)\n",
    "        #print('Rewards:', batch_rewards)\n",
    "        \n",
    "        return batch_states, batch_actions, batch_rewards\n",
    "\n",
    "    def filter_batch(self,states_batch,actions_batch,rewards_batch,percentile):\n",
    "        \n",
    "        reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "        \n",
    "        #print('reward threshold:', reward_threshold)\n",
    "        \n",
    "        threshold_cnt = 0\n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                threshold_cnt += 1\n",
    "                \n",
    "        elite_states = np.array([], dtype=float)\n",
    "        elite_actions = np.array([], dtype=int)\n",
    "        \n",
    "        elite_indices = []\n",
    "        \n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                elite_indices.append(i)\n",
    "        \n",
    "        elite_indices = np.asarray(elite_indices)\n",
    "        #print('indices', elite_indices)\n",
    "        \n",
    "        obs_num = np.shape(env.observation_space)[0]\n",
    "        #print('obs num:', obs_num)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(len(states_batch[i])):\n",
    "                    \n",
    "                if j == 0:\n",
    "                    #print('%d batch, %d count:' % (i,j) , states_batch[i][j][0])\n",
    "                    #print('%d batch, %d count:' % (i,j) , actions_batch[i][j])\n",
    "                    \n",
    "                   # for k in range(obs_num):\n",
    "                    #    elite_states = np.append(elite_states, states_batch[0][0][0][k])\n",
    "                        #print('elite states_iteration', elite_states)\n",
    "                        #print('%d batch, %d count:' % (i,j) , states_batch[i][j][0][k])\n",
    "                        \n",
    "                    elite_states = np.append(elite_states, states_batch[i][0][0]) \n",
    "                    elite_actions = np.append(elite_actions, actions_batch[0][0])\n",
    "                    #print('el_ac-> j:', elite_actions)\n",
    "                else:\n",
    "                    #print('%d batch, %d count:' % (i,j) , states_batch[i][j])\n",
    "                    #print('%d batch, %d count:' % (i,j) , actions_batch[i][j])\n",
    "                    #for k in range(obs_num):\n",
    "                        #elite_states = np.append(elite_states, states_batch[i][j][k])\n",
    "                        #print('%d batch, %d count:' % (i,j) , states_batch[i][j][k])\n",
    "                    elite_states = np.append(elite_states, states_batch[i][j])\n",
    "                    elite_actions = np.append(elite_actions, actions_batch[i][j])\n",
    "                    #elite_states.append(states_batch[i][j])\n",
    "                    #elite_actions.append(actions_batch[i][j])\n",
    "                \n",
    "        #print('elite_actions shape:',elite_actions.shape)\n",
    "        #print('elite_actions:',elite_actions)\n",
    "        return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = np.shape(env.observation_space)[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#print(n_states, n_actions)\n",
    "\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.00000, reward_mean=-1.99000, reward_threshold=-1.99000\n",
      "1: loss=0.00000, reward_mean=-1.99000, reward_threshold=-1.99000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b7e1fa998c8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#generate new sessions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mbatch_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#print('batch_states', np.asarray(batch_states).reshape(-1,4))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-df04a3323969>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[1;34m(self, env, batch_size, t_max)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m#print('action:',a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[0mnew_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\steering-optimizer\\steering_optimizer\\envs\\github\\steering-optimizer\\steering_optimizer\\envs\\optimizer_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;31m# Storing the angle values corresponding to the rack positions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0ml_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marm_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mr_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marm_ca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrack_travel\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4692\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4693\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4694\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "stats1 = np.array([])\n",
    "stats2 = np.array([])\n",
    "stats3 = np.array([])\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = net.generate_batch(env, batch_size, t_max)\n",
    "    \n",
    "    #print('batch_states', np.asarray(batch_states).reshape(-1,4))\n",
    "\n",
    "    elite_states, elite_actions = net.filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    #print('el_states', elite_states)\n",
    "    elite_states = elite_states.reshape(-1,4)\n",
    "    \n",
    "    #print('el_states: reshaped', elite_states)\n",
    "    #print('el_actions', elite_actions)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "\n",
    "    action_scores_v = net(tensor_states)\n",
    "    \n",
    "    #print(action_scores_v)\n",
    "    \n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.5f, reward_mean=%.5f, reward_threshold=%.5f\"% (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    stats1 = np.append(stats1, loss_v.item())\n",
    "    stats2 = np.append(stats2, mean_reward)\n",
    "    stats3 = np.append(stats3, threshold)\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stats1)\n",
    "plt.figure()\n",
    "plt.plot(stats2)\n",
    "plt.figure()\n",
    "plt.plot(stats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
