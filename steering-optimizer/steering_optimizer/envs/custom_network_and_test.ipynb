{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/asus_strix_scar/Anaconda3/Lib/site-packages/gym/envs/steering-optimizer/steering_optimizer/envs/github/steering-optimizer\n",
      "Requirement already satisfied: gym in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from steering-optimizer==0.0.1) (0.15.4)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (4.1.2.30)\n",
      "Requirement already satisfied: six in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.16.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: future in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym->steering-optimizer==0.0.1) (0.17.1)\n",
      "Installing collected packages: steering-optimizer\n",
      "  Found existing installation: steering-optimizer 0.0.1\n",
      "    Uninstalling steering-optimizer-0.0.1:\n",
      "      Successfully uninstalled steering-optimizer-0.0.1\n",
      "  Running setup.py develop for steering-optimizer\n",
      "Successfully installed steering-optimizer\n"
     ]
    }
   ],
   "source": [
    "!pip install -e C:\\Users\\asus_strix_scar\\Anaconda3\\Lib\\site-packages\\gym\\envs\\steering-optimizer\\steering_optimizer\\envs\\github\\steering-optimizer\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.env_specs.keys():\n",
    "     if 'steering_optimizer' in env:\n",
    "          print('Remove {} from registry\".format(env)')\n",
    "          del gym.envs.registry.env_specs[env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='steering_optimizer-v0',\n",
    "    entry_point='steering_optimizer.envs:StrOptEnv',# kwargs={'WB': 1900, 'TW': 1200, 'KP': 150,'tr_min': 4000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"steering_optimizer-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed parameters\n",
    "\n",
    "#Wheelbase, track width and kingpin distance\n",
    "\n",
    "WB = 1900\n",
    "TW = 1200\n",
    "KP = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "session_size = 200\n",
    "t_max = 200\n",
    "percentile = 80\n",
    "hidden_size = 30\n",
    "learning_rate = 0.01\n",
    "completion_score = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def generate_batch(self, env, batch_size, t_max):\n",
    "        \n",
    "        activation_1 = nn.Softmax(dim=1)\n",
    "        batch_actions, batch_states, batch_rewards = [],[],[]\n",
    "        # Reset only once in a batch\n",
    "        s_0 = env.reset()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            states,actions = [],[]\n",
    "            total_reward = 0\n",
    "            \n",
    "            #s = env.reset()\n",
    "            \n",
    "            # Imitating reset, with the same state for every batch member\n",
    "            \n",
    "            env.state = s_0\n",
    "            \n",
    "            s = s_0\n",
    "            \n",
    "            env.steps_beyond_done = None\n",
    "            env.error = None\n",
    "            env.max_r = None\n",
    "            env.steps_since_reset = 0\n",
    "            env.total_reward = 0\n",
    "            \n",
    "            #print('state shape after reset:', s.shape)\n",
    "            s = s.reshape((1,4))\n",
    "            \n",
    "             # Scaling the state width TW\n",
    "            s = s/TW*2\n",
    "            #print('state shape after reshape:', s.shape)\n",
    "            \n",
    "            count = 0\n",
    "            #print('batch:',b)\n",
    "            \n",
    "            for t in range(t_max):\n",
    "                count +=1\n",
    "                \n",
    "                s_v = torch.FloatTensor([s]).reshape(((1,4)))\n",
    "                #print('state_shape',s_v.shape)\n",
    "                #s_v = torch.FloatTensor([s])\n",
    "                \n",
    "                #print('state: x',s_v)\n",
    "                \n",
    "                act_probs_v = activation_1(self.forward(s_v))\n",
    "                act_probs = act_probs_v.data.numpy()[0]\n",
    "                choice_num = len(act_probs)\n",
    "                #print('num:',choice_num)\n",
    "                a = np.random.choice(choice_num, p=act_probs)\n",
    "                #print('action:',a)\n",
    "\n",
    "                new_s, r, done, info = env.step(a)\n",
    "\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                \n",
    "                #print('new state',new_s)\n",
    "                \n",
    "                total_reward += r\n",
    "                \n",
    "                #print('total reward on episode:', total_reward)\n",
    "                 # Scaling the state width TW               \n",
    "                s = new_s/TW*2\n",
    "            \n",
    "                \n",
    "            #print('Batch member done')\n",
    "            batch_actions.append(actions)\n",
    "            #print('Batch actions:', batch_actions[-1])\n",
    "            #print('total reward on episode:', total_reward)\n",
    "            \n",
    "            batch_states.append(states)\n",
    "            batch_rewards.append(total_reward)\n",
    "        \n",
    "        #print('Batch generated')\n",
    "        #print('States:', batch_states)\n",
    "        #print('Episode Actions:', batch_actions)\n",
    "        #print('Rewards:', batch_rewards)\n",
    "        \n",
    "        return batch_states, batch_actions, batch_rewards\n",
    "\n",
    "    def filter_batch(self,states_batch,actions_batch,rewards_batch,percentile):\n",
    "        \n",
    "        reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "        \n",
    "        #print('reward threshold:', reward_threshold)\n",
    "        \n",
    "        threshold_cnt = 0\n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                threshold_cnt += 1\n",
    "                \n",
    "        elite_states = np.array([], dtype=float)\n",
    "        elite_actions = np.array([], dtype=int)\n",
    "        \n",
    "        elite_indices = []\n",
    "        \n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                elite_indices.append(i)\n",
    "        \n",
    "        elite_indices = np.asarray(elite_indices)\n",
    "        #print('indices', elite_indices)\n",
    "        \n",
    "        obs_num = np.shape(env.observation_space)[0]\n",
    "        #print('obs num:', obs_num)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(len(states_batch[i])):\n",
    "                    \n",
    "                if j == 0:\n",
    "                    #print('%d batch, %d count:' % (i,j) , states_batch[i][j][0])\n",
    "                    #print('%d batch, %d count:' % (i,j) , actions_batch[i][j])\n",
    "                    \n",
    "                   # for k in range(obs_num):\n",
    "                    #    elite_states = np.append(elite_states, states_batch[0][0][0][k])\n",
    "                        #print('elite states_iteration', elite_states)\n",
    "                        #print('%d batch, %d count:' % (i,j) , states_batch[i][j][0][k])\n",
    "                        \n",
    "                    elite_states = np.append(elite_states, states_batch[i][0][0]) \n",
    "                    elite_actions = np.append(elite_actions, actions_batch[0][0])\n",
    "                    #print('el_ac-> j:', elite_actions)\n",
    "                else:\n",
    "                    #print('%d batch, %d count:' % (i,j) , states_batch[i][j])\n",
    "                    #print('%d batch, %d count:' % (i,j) , actions_batch[i][j])\n",
    "                    #for k in range(obs_num):\n",
    "                        #elite_states = np.append(elite_states, states_batch[i][j][k])\n",
    "                        #print('%d batch, %d count:' % (i,j) , states_batch[i][j][k])\n",
    "                    elite_states = np.append(elite_states, states_batch[i][j])\n",
    "                    elite_actions = np.append(elite_actions, actions_batch[i][j])\n",
    "                    #elite_states.append(states_batch[i][j])\n",
    "                    #elite_actions.append(actions_batch[i][j])\n",
    "                \n",
    "        #print('elite_actions shape:',elite_actions.shape)\n",
    "        #print('elite_actions:',elite_actions)\n",
    "        return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = np.shape(env.observation_space)[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#print(n_states, n_actions)\n",
    "\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=2.20654, reward_mean=-0.00900, reward_threshold=0.01000\n",
      "1: loss=2.16645, reward_mean=-0.00620, reward_threshold=0.03000\n",
      "2: loss=2.16342, reward_mean=-0.00660, reward_threshold=0.03000\n",
      "3: loss=2.19974, reward_mean=-0.00180, reward_threshold=0.03000\n",
      "4: loss=2.13448, reward_mean=-0.00740, reward_threshold=0.01000\n",
      "5: loss=2.15255, reward_mean=0.00440, reward_threshold=0.03000\n",
      "6: loss=2.16826, reward_mean=0.00960, reward_threshold=0.03000\n",
      "7: loss=2.08368, reward_mean=0.00400, reward_threshold=0.03000\n",
      "8: loss=2.14506, reward_mean=0.00840, reward_threshold=0.03000\n",
      "9: loss=2.12496, reward_mean=0.00760, reward_threshold=0.03000\n",
      "10: loss=2.16793, reward_mean=0.00720, reward_threshold=0.03000\n",
      "11: loss=2.14058, reward_mean=0.01300, reward_threshold=0.03000\n",
      "12: loss=2.03188, reward_mean=0.01520, reward_threshold=0.05000\n",
      "13: loss=2.14357, reward_mean=0.01140, reward_threshold=0.03000\n",
      "14: loss=2.12703, reward_mean=0.01340, reward_threshold=0.03000\n",
      "15: loss=2.01296, reward_mean=0.01540, reward_threshold=0.03000\n",
      "16: loss=2.07826, reward_mean=0.01640, reward_threshold=0.05000\n",
      "17: loss=2.00889, reward_mean=0.01560, reward_threshold=0.05000\n",
      "18: loss=1.99126, reward_mean=0.01540, reward_threshold=0.03000\n",
      "19: loss=1.98547, reward_mean=0.01960, reward_threshold=0.05000\n",
      "20: loss=2.07316, reward_mean=0.01380, reward_threshold=0.03400\n",
      "21: loss=2.03770, reward_mean=0.02160, reward_threshold=0.05000\n",
      "22: loss=2.04908, reward_mean=0.02240, reward_threshold=0.05000\n",
      "23: loss=2.05450, reward_mean=0.01960, reward_threshold=0.05000\n",
      "24: loss=1.89013, reward_mean=0.02400, reward_threshold=0.05000\n",
      "25: loss=1.88648, reward_mean=0.02300, reward_threshold=0.05000\n",
      "26: loss=1.99798, reward_mean=0.02640, reward_threshold=0.05000\n",
      "27: loss=1.88823, reward_mean=0.02160, reward_threshold=0.05000\n",
      "28: loss=1.96761, reward_mean=0.03000, reward_threshold=0.05000\n",
      "29: loss=1.91696, reward_mean=0.02940, reward_threshold=0.05000\n",
      "30: loss=1.84543, reward_mean=0.02680, reward_threshold=0.05000\n",
      "31: loss=1.83497, reward_mean=0.02200, reward_threshold=0.05000\n",
      "32: loss=1.76916, reward_mean=0.03020, reward_threshold=0.05000\n",
      "33: loss=1.74607, reward_mean=0.03140, reward_threshold=0.05000\n",
      "34: loss=1.91176, reward_mean=0.03280, reward_threshold=0.05000\n",
      "35: loss=1.80612, reward_mean=0.03520, reward_threshold=0.05000\n",
      "36: loss=1.83922, reward_mean=0.03260, reward_threshold=0.05000\n",
      "37: loss=1.81795, reward_mean=0.03840, reward_threshold=0.07000\n",
      "38: loss=1.63063, reward_mean=0.03880, reward_threshold=0.07000\n",
      "39: loss=1.64692, reward_mean=0.03880, reward_threshold=0.05000\n",
      "40: loss=1.84382, reward_mean=0.03820, reward_threshold=0.05000\n",
      "41: loss=1.88816, reward_mean=0.04280, reward_threshold=0.07000\n",
      "42: loss=1.79808, reward_mean=0.03800, reward_threshold=0.07000\n",
      "43: loss=1.84765, reward_mean=0.03900, reward_threshold=0.07000\n",
      "44: loss=1.78778, reward_mean=0.04260, reward_threshold=0.07000\n",
      "45: loss=1.72240, reward_mean=0.03640, reward_threshold=0.05000\n",
      "46: loss=1.69398, reward_mean=0.03600, reward_threshold=0.05000\n",
      "47: loss=1.78939, reward_mean=0.04240, reward_threshold=0.07000\n",
      "48: loss=1.81430, reward_mean=0.03960, reward_threshold=0.07000\n",
      "49: loss=1.85236, reward_mean=0.03960, reward_threshold=0.07000\n",
      "50: loss=1.84970, reward_mean=0.04420, reward_threshold=0.07000\n",
      "51: loss=1.89398, reward_mean=0.04180, reward_threshold=0.07000\n",
      "52: loss=1.73971, reward_mean=0.04140, reward_threshold=0.07000\n",
      "53: loss=1.94340, reward_mean=0.04340, reward_threshold=0.07000\n",
      "54: loss=1.92664, reward_mean=0.03820, reward_threshold=0.05400\n",
      "55: loss=1.91805, reward_mean=0.03940, reward_threshold=0.07000\n",
      "56: loss=1.96066, reward_mean=0.03880, reward_threshold=0.05400\n",
      "57: loss=1.97119, reward_mean=0.03720, reward_threshold=0.05000\n",
      "58: loss=2.08239, reward_mean=0.03580, reward_threshold=0.05000\n",
      "59: loss=1.84282, reward_mean=0.03760, reward_threshold=0.07000\n",
      "60: loss=1.95669, reward_mean=0.03140, reward_threshold=0.05000\n",
      "61: loss=1.98710, reward_mean=0.03360, reward_threshold=0.05000\n",
      "62: loss=2.07007, reward_mean=0.02480, reward_threshold=0.05000\n",
      "63: loss=1.96946, reward_mean=0.03120, reward_threshold=0.05000\n",
      "64: loss=2.04784, reward_mean=0.02860, reward_threshold=0.05000\n",
      "65: loss=2.04468, reward_mean=0.02640, reward_threshold=0.05000\n",
      "66: loss=2.00531, reward_mean=0.02900, reward_threshold=0.05000\n",
      "67: loss=2.02787, reward_mean=0.02560, reward_threshold=0.05000\n",
      "68: loss=1.95517, reward_mean=0.02680, reward_threshold=0.05000\n",
      "69: loss=2.03652, reward_mean=0.02960, reward_threshold=0.05000\n",
      "70: loss=2.08475, reward_mean=0.02700, reward_threshold=0.05000\n",
      "71: loss=2.07478, reward_mean=0.02560, reward_threshold=0.05000\n",
      "72: loss=2.02738, reward_mean=0.02700, reward_threshold=0.05000\n",
      "73: loss=2.08563, reward_mean=0.02560, reward_threshold=0.05000\n",
      "74: loss=2.02533, reward_mean=0.02780, reward_threshold=0.05000\n",
      "75: loss=2.04247, reward_mean=0.02160, reward_threshold=0.05000\n",
      "76: loss=2.06752, reward_mean=0.02940, reward_threshold=0.05000\n",
      "77: loss=2.07396, reward_mean=0.02220, reward_threshold=0.05000\n",
      "78: loss=2.02297, reward_mean=0.02660, reward_threshold=0.05000\n",
      "79: loss=2.17057, reward_mean=0.02120, reward_threshold=0.05000\n",
      "80: loss=2.02675, reward_mean=0.01960, reward_threshold=0.05000\n",
      "81: loss=2.03083, reward_mean=0.02280, reward_threshold=0.05000\n",
      "82: loss=2.03077, reward_mean=0.02200, reward_threshold=0.05000\n",
      "83: loss=2.00995, reward_mean=0.02560, reward_threshold=0.05000\n",
      "84: loss=2.03985, reward_mean=0.02440, reward_threshold=0.05000\n",
      "85: loss=2.07039, reward_mean=0.02140, reward_threshold=0.05000\n",
      "86: loss=2.04842, reward_mean=0.02380, reward_threshold=0.05000\n",
      "87: loss=2.04778, reward_mean=0.02500, reward_threshold=0.05000\n",
      "88: loss=1.99227, reward_mean=0.02580, reward_threshold=0.05000\n",
      "89: loss=2.06534, reward_mean=0.02760, reward_threshold=0.05000\n",
      "90: loss=1.99405, reward_mean=0.02740, reward_threshold=0.05000\n",
      "91: loss=2.02951, reward_mean=0.02380, reward_threshold=0.05000\n",
      "92: loss=1.96484, reward_mean=0.02940, reward_threshold=0.05000\n",
      "93: loss=1.98168, reward_mean=0.02440, reward_threshold=0.05000\n",
      "94: loss=2.02866, reward_mean=0.02480, reward_threshold=0.05000\n",
      "95: loss=1.95047, reward_mean=0.02820, reward_threshold=0.05000\n",
      "96: loss=1.93037, reward_mean=0.03120, reward_threshold=0.05000\n",
      "97: loss=1.96831, reward_mean=0.02500, reward_threshold=0.05000\n",
      "98: loss=1.95496, reward_mean=0.02860, reward_threshold=0.05000\n",
      "99: loss=1.97678, reward_mean=0.02820, reward_threshold=0.05000\n",
      "100: loss=1.90410, reward_mean=0.03220, reward_threshold=0.05000\n",
      "101: loss=1.89046, reward_mean=0.03820, reward_threshold=0.05400\n",
      "102: loss=1.88379, reward_mean=0.03100, reward_threshold=0.05000\n",
      "103: loss=1.98703, reward_mean=0.03660, reward_threshold=0.07000\n",
      "104: loss=1.87474, reward_mean=0.03480, reward_threshold=0.05400\n",
      "105: loss=1.87273, reward_mean=0.03880, reward_threshold=0.07000\n",
      "106: loss=1.85465, reward_mean=0.03700, reward_threshold=0.05000\n",
      "107: loss=1.84648, reward_mean=0.03680, reward_threshold=0.07000\n",
      "108: loss=1.93427, reward_mean=0.03480, reward_threshold=0.05000\n",
      "109: loss=1.79296, reward_mean=0.03740, reward_threshold=0.07000\n",
      "110: loss=1.89042, reward_mean=0.03860, reward_threshold=0.05000\n",
      "111: loss=1.82068, reward_mean=0.04800, reward_threshold=0.07000\n",
      "112: loss=1.77908, reward_mean=0.03820, reward_threshold=0.05000\n",
      "113: loss=1.87202, reward_mean=0.04300, reward_threshold=0.07000\n",
      "114: loss=1.74339, reward_mean=0.04520, reward_threshold=0.07000\n",
      "115: loss=1.72311, reward_mean=0.04160, reward_threshold=0.07000\n",
      "116: loss=1.73235, reward_mean=0.04420, reward_threshold=0.07000\n",
      "117: loss=1.91336, reward_mean=0.03740, reward_threshold=0.07000\n",
      "118: loss=1.70505, reward_mean=0.04440, reward_threshold=0.07000\n",
      "119: loss=1.84548, reward_mean=0.03700, reward_threshold=0.05000\n",
      "120: loss=1.75234, reward_mean=0.04660, reward_threshold=0.07000\n",
      "121: loss=1.71915, reward_mean=0.04160, reward_threshold=0.07000\n",
      "122: loss=1.70125, reward_mean=0.04080, reward_threshold=0.07000\n",
      "123: loss=1.66137, reward_mean=0.04620, reward_threshold=0.07000\n",
      "124: loss=1.71450, reward_mean=0.03760, reward_threshold=0.07000\n",
      "125: loss=1.66374, reward_mean=0.04100, reward_threshold=0.07000\n",
      "126: loss=1.81771, reward_mean=0.03940, reward_threshold=0.07000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127: loss=1.59482, reward_mean=0.04440, reward_threshold=0.07000\n",
      "128: loss=1.60386, reward_mean=0.04620, reward_threshold=0.07000\n",
      "129: loss=1.66077, reward_mean=0.04340, reward_threshold=0.07000\n",
      "130: loss=1.67327, reward_mean=0.04220, reward_threshold=0.07000\n",
      "131: loss=1.59008, reward_mean=0.04540, reward_threshold=0.07000\n",
      "132: loss=1.55175, reward_mean=0.04500, reward_threshold=0.07000\n",
      "133: loss=1.48646, reward_mean=0.04800, reward_threshold=0.07000\n",
      "134: loss=1.50170, reward_mean=0.04800, reward_threshold=0.07000\n",
      "135: loss=1.49644, reward_mean=0.05120, reward_threshold=0.07000\n",
      "136: loss=1.44728, reward_mean=0.05000, reward_threshold=0.07000\n",
      "137: loss=1.53395, reward_mean=0.05640, reward_threshold=0.07000\n",
      "138: loss=1.50014, reward_mean=0.05820, reward_threshold=0.07000\n",
      "139: loss=1.39709, reward_mean=0.05300, reward_threshold=0.07000\n",
      "140: loss=1.35775, reward_mean=0.05560, reward_threshold=0.07000\n",
      "141: loss=1.33892, reward_mean=0.05700, reward_threshold=0.07000\n",
      "142: loss=1.22970, reward_mean=0.06160, reward_threshold=0.09000\n",
      "143: loss=1.19844, reward_mean=0.05900, reward_threshold=0.07400\n",
      "144: loss=1.27080, reward_mean=0.05740, reward_threshold=0.07000\n",
      "145: loss=1.12193, reward_mean=0.06160, reward_threshold=0.09000\n",
      "146: loss=1.13222, reward_mean=0.06300, reward_threshold=0.09000\n",
      "147: loss=1.09630, reward_mean=0.06020, reward_threshold=0.09000\n",
      "148: loss=1.30182, reward_mean=0.06560, reward_threshold=0.09000\n",
      "149: loss=0.92767, reward_mean=0.06900, reward_threshold=0.09000\n",
      "150: loss=0.92385, reward_mean=0.06920, reward_threshold=0.09000\n",
      "151: loss=1.09673, reward_mean=0.07180, reward_threshold=0.09000\n",
      "152: loss=0.93501, reward_mean=0.06860, reward_threshold=0.09000\n",
      "153: loss=0.92399, reward_mean=0.07000, reward_threshold=0.09000\n",
      "154: loss=0.83734, reward_mean=0.06760, reward_threshold=0.09000\n",
      "155: loss=0.78977, reward_mean=0.07200, reward_threshold=0.09000\n",
      "156: loss=1.07097, reward_mean=0.06800, reward_threshold=0.09000\n",
      "157: loss=0.85818, reward_mean=0.07120, reward_threshold=0.09000\n",
      "158: loss=0.81491, reward_mean=0.07020, reward_threshold=0.09000\n",
      "159: loss=0.99582, reward_mean=0.07120, reward_threshold=0.09000\n",
      "160: loss=0.80209, reward_mean=0.07040, reward_threshold=0.09000\n",
      "161: loss=0.81208, reward_mean=0.07180, reward_threshold=0.09000\n",
      "162: loss=0.79660, reward_mean=0.06920, reward_threshold=0.09000\n",
      "163: loss=0.77250, reward_mean=0.06880, reward_threshold=0.09000\n",
      "164: loss=0.79553, reward_mean=0.06920, reward_threshold=0.09000\n",
      "165: loss=1.04304, reward_mean=0.06640, reward_threshold=0.09000\n",
      "166: loss=0.75982, reward_mean=0.07140, reward_threshold=0.09000\n",
      "167: loss=0.79173, reward_mean=0.06860, reward_threshold=0.09000\n",
      "168: loss=0.95018, reward_mean=0.06540, reward_threshold=0.09000\n",
      "169: loss=0.76000, reward_mean=0.07040, reward_threshold=0.09000\n",
      "170: loss=0.76245, reward_mean=0.06840, reward_threshold=0.09000\n",
      "171: loss=0.74883, reward_mean=0.06920, reward_threshold=0.09000\n",
      "172: loss=0.77525, reward_mean=0.06300, reward_threshold=0.09000\n",
      "173: loss=0.83257, reward_mean=0.06640, reward_threshold=0.09000\n",
      "174: loss=1.06689, reward_mean=0.06840, reward_threshold=0.09000\n",
      "175: loss=0.72612, reward_mean=0.06420, reward_threshold=0.09000\n",
      "176: loss=1.08096, reward_mean=0.06560, reward_threshold=0.09000\n",
      "177: loss=0.82301, reward_mean=0.06440, reward_threshold=0.09000\n",
      "178: loss=0.94789, reward_mean=0.06560, reward_threshold=0.09000\n",
      "179: loss=0.79261, reward_mean=0.06460, reward_threshold=0.09000\n",
      "180: loss=0.83840, reward_mean=0.06320, reward_threshold=0.09000\n",
      "181: loss=1.11089, reward_mean=0.06020, reward_threshold=0.09000\n",
      "182: loss=0.86484, reward_mean=0.05880, reward_threshold=0.07000\n",
      "183: loss=1.08194, reward_mean=0.05840, reward_threshold=0.09000\n"
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "stats1 = np.array([])\n",
    "stats2 = np.array([])\n",
    "stats3 = np.array([])\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = net.generate_batch(env, batch_size, 10)\n",
    "    \n",
    "    #print('batch_states', np.asarray(batch_states).reshape(-1,4))\n",
    "\n",
    "    elite_states, elite_actions = net.filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    #print('el_states', elite_states)\n",
    "    elite_states = elite_states.reshape(-1,4)\n",
    "    \n",
    "    #print('el_states: reshaped', elite_states)\n",
    "    #print('el_actions', elite_actions)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "\n",
    "    action_scores_v = net(tensor_states)\n",
    "    \n",
    "    #print(action_scores_v)\n",
    "    \n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.5f, reward_mean=%.5f, reward_threshold=%.5f\"% (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    stats1 = np.append(stats1, loss_v.item())\n",
    "    stats2 = np.append(stats2, mean_reward)\n",
    "    stats3 = np.append(stats3, threshold)\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stats1)\n",
    "plt.figure()\n",
    "plt.plot(stats2)\n",
    "plt.figure()\n",
    "plt.plot(stats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
