{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/asus_strix_scar/Anaconda3/Lib/site-packages/gym/envs/steering-optimizer/steering_optimizer/envs/github/steering-optimizer\n",
      "Requirement already satisfied: gym in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from steering-optimizer==0.0.1) (0.15.4)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.16.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (4.1.2.30)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from gym->steering-optimizer==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: future in c:\\users\\asus_strix_scar\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym->steering-optimizer==0.0.1) (0.17.1)\n",
      "Installing collected packages: steering-optimizer\n",
      "  Found existing installation: steering-optimizer 0.0.1\n",
      "    Uninstalling steering-optimizer-0.0.1:\n",
      "      Successfully uninstalled steering-optimizer-0.0.1\n",
      "  Running setup.py develop for steering-optimizer\n",
      "Successfully installed steering-optimizer\n"
     ]
    }
   ],
   "source": [
    "!pip install -e C:\\Users\\asus_strix_scar\\Anaconda3\\Lib\\site-packages\\gym\\envs\\steering-optimizer\\steering_optimizer\\envs\\github\\steering-optimizer\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.env_specs.keys():\n",
    "     if 'steering_optimizer' in env:\n",
    "          print('Remove {} from registry\".format(env)')\n",
    "          del gym.envs.registry.env_specs[env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='steering_optimizer-v0',\n",
    "    entry_point='steering_optimizer.envs:StrOptEnv',# kwargs={'WB': 1900, 'TW': 1200, 'KP': 150,'tr_min': 4000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"steering_optimizer-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed parameters\n",
    "\n",
    "#Wheelbase, track width and kingpin distance\n",
    "\n",
    "WB = 1900\n",
    "TW = 1200\n",
    "KP = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "session_size = 100\n",
    "t_max = 300\n",
    "percentile = 90\n",
    "hidden_size = 20\n",
    "learning_rate = 0.01\n",
    "completion_score = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "        \n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activation = nn.Softmax(dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc0(x))\n",
    "        \n",
    "        return activation(self.fc1(x))\n",
    "\n",
    "    def generate_batch(self, env, batch_size, t_max):\n",
    "        \n",
    "        #activation_1 = nn.Softmax(dim=1)\n",
    "        batch_actions, batch_states, batch_rewards = [],[],[]\n",
    "        # Reset only once in a batch\n",
    "        s_0 = env.reset()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            states,actions = [],[]\n",
    "            total_reward = 0\n",
    "            \n",
    "            s = env.reset()\n",
    "            # Imitating reset, with the same state for every batch member\n",
    "            #env.state = s_0\n",
    "            \n",
    "            #s = s_0\n",
    "            \n",
    "            #env.steps_beyond_done = None\n",
    "            #env.error = None\n",
    "            #env.max_r = None\n",
    "            #env.steps_since_reset = 0\n",
    "            #env.total_reward = 0\n",
    "            \n",
    "            #print('state shape after reset:', s.shape)\n",
    "            s = s.reshape((1,4))\n",
    "            \n",
    "            #print('state shape after reshape:', s.shape)\n",
    "            \n",
    "            # step counter\n",
    "            count = 0\n",
    "            \n",
    "            for t in range(t_max):\n",
    "                count +=1\n",
    "                \n",
    "                # Scaling state to be within -1 and 1\n",
    "                s = s/TW*2\n",
    "                \n",
    "                s_v = torch.FloatTensor([s]).reshape(((1,4)))\n",
    "                #print('state_shape',s_v.shape)\n",
    "                #s_v = torch.FloatTensor([s])\n",
    "                \n",
    "                #print('state: x',s_v)\n",
    "                #act_probs_v = activation_1(self.forward(s_v))\n",
    "                act_probs_v = self.forward(s_v)\n",
    "                act_probs = act_probs_v.data.numpy()[0]\n",
    "                choice_num = len(act_probs)\n",
    "                #print('num:',choice_num)\n",
    "                \n",
    "                ## Making only \n",
    "                #env.steps_since_reset += 1\n",
    "                \n",
    "                #r = -1\n",
    "                \n",
    "                #while r <= 0:\n",
    "                #    env.steps_since_reset -= 1\n",
    "                #    a = np.random.choice(choice_num, p=act_probs)\n",
    "                #    \n",
    "                #    new_s, r, done, info = env.step(a)\n",
    "                # \n",
    "                \n",
    "                a = np.random.choice(choice_num, p=act_probs)\n",
    "                #a = np.random.choice(choice_num)\n",
    "                   \n",
    "                new_s, r, done, info = env.step(a)\n",
    "\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                \n",
    "                #print('new state',new_s)\n",
    "                \n",
    "                total_reward += r\n",
    "                \n",
    "                #print('action:',a, 'reward: ', r, 'error:', env.error, 'total reward:', total_reward)\n",
    "                \n",
    "                # print('total reward:', total_reward)\n",
    "                # Getting the new state               \n",
    "                s = new_s/TW*2\n",
    "                \n",
    "                # After done, no more steps\n",
    "                if done:\n",
    "                    #print('DONE happened')\n",
    "                    break\n",
    "            \n",
    "            batch_actions.append(actions)\n",
    "            #print('Batch member done')\n",
    "            #print('Batch actions:', batch_actions[-1])\n",
    "            #print('total reward on episode:', total_reward)\n",
    "            \n",
    "            batch_states.append(states)\n",
    "            batch_rewards.append(total_reward)\n",
    "        \n",
    "        #print('Batch generated')\n",
    "        #print('States:', batch_states)\n",
    "        #print('Episode Actions:', batch_actions)\n",
    "        #print('Rewards:', batch_rewards)\n",
    "        \n",
    "        return batch_states, batch_actions, batch_rewards\n",
    "\n",
    "    def filter_batch(self, states_batch, actions_batch, rewards_batch, percentile):\n",
    "        \n",
    "        reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "        \n",
    "        #print('reward threshold:', reward_threshold)\n",
    "        \n",
    "        threshold_cnt = 0\n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                threshold_cnt += 1\n",
    "                \n",
    "        elite_states = np.array([], dtype=float)\n",
    "        elite_actions = np.array([], dtype=int)\n",
    "        \n",
    "        elite_indices = []\n",
    "        \n",
    "        for i in range(len(rewards_batch)):\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                elite_indices.append(i)\n",
    "        \n",
    "        elite_indices = np.asarray(elite_indices)\n",
    "        #print('indices', elite_indices)\n",
    "        \n",
    "        obs_num = np.shape(env.observation_space)[0]\n",
    "        #print('obs num:', obs_num)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(len(states_batch[i])):\n",
    "                    \n",
    "                if j == 0:\n",
    "                    elite_states = np.append(elite_states, states_batch[i][0][0]) \n",
    "                    elite_actions = np.append(elite_actions, actions_batch[0][0])\n",
    "\n",
    "                else:\n",
    "                    elite_states = np.append(elite_states, states_batch[i][j])\n",
    "                    elite_actions = np.append(elite_actions, actions_batch[i][j])\n",
    "\n",
    "        return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = np.shape(env.observation_space)[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#print(n_states, n_actions)\n",
    "\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12 0.12 0.09 0.12 0.1  0.11 0.08 0.15 0.12]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Check the initialized network\n",
    "\n",
    "s = np.asarray([-100, -200, -410, -50])\n",
    "\n",
    "s = s/TW*2\n",
    "\n",
    "s_v = torch.FloatTensor([s]).reshape(((1,4)))\n",
    "\n",
    "next_step = net.forward(s_v)\n",
    "\n",
    "print(np.round((next_step[0]).detach().numpy(),decimals=2))\n",
    "print(np.argmax((next_step[0]).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Array length error\n",
      "Here is the problem! -> not every angle value is unique\n",
      "r_array_mod [ 0.00000000e+00  0.00000000e+00 -8.65223728e-06 -1.30459610e-05\n",
      " -1.74866805e-05 -2.19759332e-05 -2.65153509e-05 -3.11066504e-05\n",
      " -3.57516549e-05 -4.04522934e-05 -4.52106093e-05 -5.00287883e-05\n",
      " -5.49091414e-05 -5.98541417e-05 -6.48664284e-05 -6.99488261e-05\n",
      " -7.51043640e-05 -8.03362889e-05 -8.56481037e-05 -9.10435802e-05\n",
      " -9.65268065e-05 -1.02102204e-04 -1.07774586e-04 -1.13549197e-04\n",
      " -1.19431765e-04 -1.25428593e-04 -1.31546600e-04 -1.37793448e-04\n",
      " -1.44177637e-04 -1.50708646e-04 -1.57397092e-04 -1.64254942e-04\n",
      " -1.71295764e-04 -1.78535027e-04 -1.85990534e-04 -1.93682892e-04\n",
      " -2.01636229e-04 -2.09879055e-04 -2.18445489e-04 -2.27376937e-04\n",
      " -2.36724455e-04 -2.46552256e-04 -2.56942979e-04 -2.68006112e-04\n",
      " -2.79892011e-04 -2.92817141e-04 -3.07113755e-04 -3.23341121e-04\n",
      " -3.42589761e-04 -3.67675027e-04]\n",
      "0: loss=2.19518, reward_mean=17.72094, reward_threshold=67.14365\n",
      "1: loss=2.19379, reward_mean=22.66436, reward_threshold=71.96600\n",
      "2: loss=2.19330, reward_mean=17.80946, reward_threshold=68.16875\n",
      "3: loss=2.19245, reward_mean=17.76592, reward_threshold=62.19500\n",
      "Done: Array length error\n",
      "Here is the problem! -> not every angle value is unique\n",
      "r_array_mod [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.03319016e-06 -7.50623497e-06 -8.96593821e-06 -1.04126760e-05\n",
      " -1.18467774e-05 -1.32685632e-05 -1.46783552e-05 -1.60764471e-05\n",
      " -1.74631205e-05 -1.88386518e-05 -2.02033178e-05 -2.15573666e-05\n",
      " -2.29010271e-05 -2.42345548e-05 -2.55581582e-05 -2.68720740e-05\n",
      " -2.81764941e-05 -2.94716415e-05 -3.07576963e-05 -3.20348427e-05\n",
      " -3.33032791e-05 -3.45631698e-05 -3.58146927e-05 -3.70580041e-05\n",
      " -3.82932728e-05 -3.95206478e-05 -4.07402668e-05 -4.19523021e-05\n",
      " -4.31568684e-05 -4.43541181e-05 -4.55441815e-05 -4.67271664e-05\n",
      " -4.79032288e-05 -4.90724648e-05 -5.02350002e-05 -5.13909530e-05\n",
      " -5.25404301e-05 -5.36835346e-05 -5.48203721e-05 -5.59510579e-05\n",
      " -5.70756782e-05 -5.81943249e-05 -5.93071094e-05 -6.04140994e-05\n",
      " -6.15154081e-05 -6.26111089e-05]\n",
      "4: loss=2.19016, reward_mean=15.63041, reward_threshold=66.83285\n",
      "5: loss=2.18784, reward_mean=18.10647, reward_threshold=65.42000\n",
      "6: loss=2.18623, reward_mean=16.04471, reward_threshold=63.40300\n",
      "7: loss=2.18345, reward_mean=16.21883, reward_threshold=58.07000\n",
      "8: loss=2.17985, reward_mean=11.55538, reward_threshold=66.22520\n",
      "9: loss=2.17706, reward_mean=15.73547, reward_threshold=57.68000\n",
      "10: loss=2.17290, reward_mean=13.22840, reward_threshold=57.42695\n",
      "11: loss=2.16912, reward_mean=13.15354, reward_threshold=55.45195\n",
      "12: loss=2.16324, reward_mean=11.73306, reward_threshold=54.57105\n",
      "13: loss=2.15771, reward_mean=12.15341, reward_threshold=52.42790\n",
      "14: loss=2.15176, reward_mean=10.12258, reward_threshold=53.30500\n",
      "15: loss=2.14456, reward_mean=14.92809, reward_threshold=55.31000\n",
      "16: loss=2.14016, reward_mean=8.01720, reward_threshold=35.86300\n",
      "17: loss=2.13212, reward_mean=7.40815, reward_threshold=43.73240\n",
      "18: loss=2.12413, reward_mean=8.90035, reward_threshold=37.49400\n",
      "19: loss=2.11984, reward_mean=5.06520, reward_threshold=36.34635\n",
      "20: loss=2.11078, reward_mean=2.09252, reward_threshold=29.26535\n",
      "21: loss=2.10309, reward_mean=3.53079, reward_threshold=26.90000\n",
      "22: loss=2.09397, reward_mean=1.87369, reward_threshold=22.45500\n",
      "Done: Array length error\n",
      "Here is the problem! -> not every angle value is unique\n",
      "r_array_mod [ 0.00000000e+00  0.00000000e+00 -9.38356793e-06 -1.40076682e-05\n",
      " -1.85883507e-05 -2.31268121e-05 -2.76241980e-05 -3.20816071e-05\n",
      " -3.65000851e-05 -4.08806331e-05 -4.52242171e-05 -4.95317488e-05\n",
      " -5.38041164e-05 -5.80421706e-05 -6.22467168e-05 -6.64185426e-05\n",
      " -7.05584033e-05 -7.46670145e-05 -7.87450805e-05 -8.27932693e-05\n",
      " -8.68122286e-05 -9.08025810e-05 -9.47649320e-05 -9.86998599e-05\n",
      " -1.02607931e-04 -1.06489681e-04 -1.10345641e-04 -1.14176315e-04\n",
      " -1.17982194e-04 -1.21763755e-04 -1.25521455e-04 -1.29255742e-04\n",
      " -1.32967053e-04 -1.36655798e-04 -1.40322391e-04 -1.43967223e-04\n",
      " -1.47590678e-04 -1.51193125e-04 -1.54774930e-04 -1.58336439e-04\n",
      " -1.61877997e-04 -1.65399935e-04 -1.68902574e-04 -1.72386227e-04\n",
      " -1.75851203e-04 -1.79297797e-04 -1.82726296e-04 -1.86136989e-04\n",
      " -1.89530142e-04 -1.92906030e-04]\n",
      "23: loss=2.08589, reward_mean=4.47020, reward_threshold=27.94100\n",
      "24: loss=2.07475, reward_mean=3.49365, reward_threshold=23.63045\n",
      "25: loss=2.06604, reward_mean=2.40792, reward_threshold=20.38255\n",
      "Done: Array length error\n",
      "Here is the problem! -> not every angle value is unique\n",
      "r_array_mod [ 0.00000000e+00  0.00000000e+00 -9.11715885e-06 -1.36099793e-05\n",
      " -1.80606129e-05 -2.24702226e-05 -2.68399252e-05 -3.11707842e-05\n",
      " -3.54638185e-05 -3.97200003e-05 -4.39402637e-05 -4.81255039e-05\n",
      " -5.22765765e-05 -5.63943064e-05 -6.04794847e-05 -6.45328703e-05\n",
      " -6.85551942e-05 -7.25471618e-05 -7.65094466e-05 -8.04427037e-05\n",
      " -8.43475622e-05 -8.82246261e-05 -9.20744822e-05 -9.58976968e-05\n",
      " -9.96948124e-05 -1.03466357e-04 -1.07212842e-04 -1.10934761e-04\n",
      " -1.14632588e-04 -1.18306786e-04 -1.21957804e-04 -1.25586074e-04\n",
      " -1.29192015e-04 -1.32776035e-04 -1.36338531e-04 -1.39879883e-04\n",
      " -1.43400465e-04 -1.46900637e-04 -1.50380751e-04 -1.53841148e-04\n",
      " -1.57282158e-04 -1.60704105e-04 -1.64107301e-04 -1.67492052e-04\n",
      " -1.70858654e-04 -1.74207395e-04 -1.77538558e-04 -1.80852417e-04\n",
      " -1.84149238e-04 -1.87429281e-04]\n",
      "26: loss=2.05383, reward_mean=0.98334, reward_threshold=17.01200\n",
      "27: loss=2.04497, reward_mean=-0.51262, reward_threshold=21.69900\n",
      "28: loss=2.03637, reward_mean=-0.35767, reward_threshold=21.91000\n",
      "29: loss=2.02053, reward_mean=1.58890, reward_threshold=27.02200\n",
      "Done: Array length error\n",
      "Here is the problem! -> not every angle value is unique\n",
      "r_array_mod [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -5.32121469e-06 -6.42037801e-06 -7.53211447e-06\n",
      " -8.65685680e-06 -9.79505593e-06 -1.09472379e-05 -1.21138721e-05\n",
      " -1.32956300e-05 -1.44930138e-05 -1.57066522e-05 -1.69373338e-05\n",
      " -1.81856581e-05 -1.94525180e-05 -2.07387559e-05 -2.20451764e-05\n",
      " -2.33728906e-05 -2.47229076e-05 -2.60964251e-05 -2.74946819e-05\n",
      " -2.89190940e-05 -3.03711227e-05 -3.18525464e-05 -3.33651522e-05\n",
      " -3.49110798e-05 -3.64924576e-05 -3.81120453e-05 -3.97725645e-05\n",
      " -4.14774801e-05 -4.32303546e-05 -4.50356434e-05 -4.68982702e-05\n",
      " -4.88241079e-05 -5.08200478e-05 -5.28943237e-05 -5.50569372e-05\n",
      " -5.73203661e-05 -5.97000982e-05 -6.22161100e-05 -6.48950364e-05\n",
      " -6.77729594e-05 -7.09027635e-05 -7.43645977e-05 -7.82939072e-05\n",
      " -8.29548480e-05 -8.90290977e-05]\n",
      "30: loss=2.00551, reward_mean=-0.66326, reward_threshold=24.99495\n",
      "31: loss=1.97495, reward_mean=-4.78253, reward_threshold=22.79000\n",
      "32: loss=1.94475, reward_mean=-5.77431, reward_threshold=26.19600\n",
      "33: loss=1.91177, reward_mean=-17.28053, reward_threshold=2.07800\n",
      "34: loss=1.86983, reward_mean=-20.40722, reward_threshold=-2.08100\n",
      "35: loss=1.82469, reward_mean=-22.21962, reward_threshold=-5.97100\n",
      "36: loss=1.78311, reward_mean=-24.70731, reward_threshold=0.05500\n",
      "37: loss=1.72044, reward_mean=-30.02010, reward_threshold=-10.00000\n",
      "38: loss=1.66624, reward_mean=-29.85991, reward_threshold=-10.00000\n",
      "39: loss=1.61317, reward_mean=-34.52050, reward_threshold=-10.00000\n",
      "40: loss=1.58105, reward_mean=-31.05420, reward_threshold=-10.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus_strix_scar\\Anaconda3\\lib\\site-packages\\gym\\envs\\steering-optimizer\\steering_optimizer\\envs\\github\\steering-optimizer\\steering_optimizer\\envs\\optimizer_env.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  chop = (angle*180/np.pi) % (np.sign(angle)*180)*np.pi/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41: loss=1.52308, reward_mean=-36.50910, reward_threshold=-10.00000\n",
      "42: loss=1.49094, reward_mean=-31.33960, reward_threshold=-10.00000\n",
      "43: loss=1.46108, reward_mean=-29.92300, reward_threshold=-10.00000\n",
      "44: loss=1.44522, reward_mean=-32.61300, reward_threshold=-10.00000\n",
      "45: loss=1.42686, reward_mean=-33.33700, reward_threshold=-10.00000\n",
      "46: loss=1.41017, reward_mean=-32.07460, reward_threshold=-10.00000\n",
      "47: loss=1.40061, reward_mean=-38.53690, reward_threshold=-10.00000\n",
      "48: loss=1.39581, reward_mean=-35.14021, reward_threshold=-10.00000\n",
      "49: loss=1.38931, reward_mean=-34.78970, reward_threshold=-10.00000\n",
      "50: loss=1.38519, reward_mean=-36.25770, reward_threshold=-10.00000\n",
      "51: loss=1.38469, reward_mean=-34.65600, reward_threshold=-10.00000\n",
      "52: loss=1.38071, reward_mean=-28.72340, reward_threshold=-10.00000\n",
      "53: loss=1.37844, reward_mean=-32.56610, reward_threshold=-10.00000\n",
      "54: loss=1.37736, reward_mean=-36.01800, reward_threshold=-10.00000\n",
      "55: loss=1.37715, reward_mean=-31.80190, reward_threshold=-10.00000\n",
      "56: loss=1.37597, reward_mean=-36.40040, reward_threshold=-10.00000\n",
      "57: loss=1.37559, reward_mean=-34.23310, reward_threshold=-10.00000\n",
      "58: loss=1.37448, reward_mean=-32.09680, reward_threshold=-10.00000\n",
      "59: loss=1.37474, reward_mean=-34.02390, reward_threshold=-10.00000\n",
      "60: loss=1.37493, reward_mean=-37.47180, reward_threshold=-10.00000\n",
      "61: loss=1.37470, reward_mean=-33.82720, reward_threshold=-10.00000\n",
      "62: loss=1.37395, reward_mean=-33.22820, reward_threshold=-10.00000\n",
      "63: loss=1.37341, reward_mean=-29.46050, reward_threshold=-10.00000\n",
      "64: loss=1.37322, reward_mean=-35.54210, reward_threshold=-10.00000\n",
      "65: loss=1.37338, reward_mean=-33.99770, reward_threshold=-10.00000\n",
      "66: loss=1.37254, reward_mean=-29.60480, reward_threshold=-10.00000\n",
      "67: loss=1.37269, reward_mean=-30.92520, reward_threshold=-10.00000\n",
      "68: loss=1.37287, reward_mean=-36.80920, reward_threshold=-10.00000\n",
      "69: loss=1.37277, reward_mean=-40.84100, reward_threshold=-10.00000\n"
     ]
    }
   ],
   "source": [
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "stats1 = np.array([])\n",
    "stats2 = np.array([])\n",
    "stats3 = np.array([])\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = net.generate_batch(env, batch_size, t_max)\n",
    "    \n",
    "    #print('batch_states', np.asarray(batch_states).reshape(-1,4))\n",
    "\n",
    "    elite_states, elite_actions = net.filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    #print('el_states', elite_states)\n",
    "    elite_states = elite_states.reshape(-1,4)\n",
    "    \n",
    "    #print('el_states: reshaped', elite_states)\n",
    "    #print('el_actions', elite_actions)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "\n",
    "    action_scores_v = net(tensor_states)\n",
    "    \n",
    "    #print(action_scores_v)\n",
    "    \n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.5f, reward_mean=%.5f, reward_threshold=%.5f\"% (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    stats1 = np.append(stats1, loss_v.item())\n",
    "    stats2 = np.append(stats2, mean_reward)\n",
    "    stats3 = np.append(stats3, threshold)\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfully completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stats1)\n",
    "plt.figure()\n",
    "plt.plot(stats2)\n",
    "plt.figure()\n",
    "plt.plot(stats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.asarray([-100, -200, -410, -50])\n",
    "\n",
    "s = s/TW*2\n",
    "\n",
    "s_v = torch.FloatTensor([s]).reshape(((1,4)))\n",
    "\n",
    "next_step = net.forward(s_v)\n",
    "\n",
    "print(np.round((next_step[0]).detach().numpy(),decimals=2))\n",
    "print(np.round((next_step[0]).detach().numpy(),decimals=0))\n",
    "print(np.argmax((next_step[0]).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"steering_optimizer-v0\"), directory=\"videos\", force=True)\n",
    "net.generate_batch(env, 1, t_max=5000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
