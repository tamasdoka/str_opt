{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimizer_env as st\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "env = st.StrOptEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "#env.step(1)\n",
    "\n",
    "#reward = np.array([])\n",
    "\n",
    "total_reward = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus_strix_scar\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after episode:  1.0800000000000005\n",
      "total reward after episode:  1.0800000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0500000000000005\n",
      "total reward after episode:  1.0600000000000005\n",
      "total reward after episode:  0.9700000000000004\n",
      "total reward after episode:  0.9700000000000004\n",
      "total reward after episode:  0.9700000000000004\n",
      "total reward after episode:  0.9700000000000004\n",
      "total reward after episode:  0.9700000000000004\n",
      "total reward after episode:  0.9700000000000004\n",
      "total reward after episode:  0.9700000000000004\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  -1.9099999999999966\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  1.0900000000000034\n",
      "total reward after episode:  10.090000000000003\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -20.92\n",
      "total reward after episode:  -17.92\n",
      "total reward after episode:  -17.92\n",
      "total reward after episode:  -17.92\n",
      "total reward after episode:  -31.930000000000007\n"
     ]
    }
   ],
   "source": [
    "for k in range (500):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    for i in range(201):\n",
    "        #act = 5\n",
    "        act = np.random.randint(0,8)\n",
    "        #valami = env.reset()\n",
    "        #env.reset()            \n",
    "        val = env.step(act)\n",
    "        #print('reward: ',val[1])\n",
    "        \n",
    "        total_reward += val[1]\n",
    "        \n",
    "    #valami = env.step(act)[0]\n",
    "    #env.save_plot(env.check_error, env.check_r)\n",
    "    #plt.plot([env.KPLX, env.state[2], env.state[0], -env.state[0]], [env.KPLY, env.state[3], env.state[1], env.state[1]])\n",
    "    #plt.show\n",
    "    #print(i,'error_mod' , env.check_error)\n",
    "    #print(i,'r_array_mod', env.check_r)\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    #print(env.state)\n",
    "\n",
    "    #print(env.rack_travel)\n",
    "    #print('total_reward: ', total_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = env.state\n",
    "\n",
    "plt.plot([env.KPLX, val[2], val[0], -val[0], -val[2], -env.KPLX], [env.KPLY, val[3], val[1], val[1], val[3], env.KPLY], 'bo')\n",
    "plt.plot([env.KPLX, val[2], val[0], -val[0], -val[2], -env.KPLX], [env.KPLY, val[3], val[1], val[1], val[3], env.KPLY])\n",
    "plt.axis('equal')\n",
    "\n",
    "env.check_error\n",
    "\n",
    "env.state = val\n",
    "\n",
    "print(env.check_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.check_error)\n",
    "print(env.check_r)\n",
    "\n",
    "print(np.trapz(env.check_error, env.check_r))\n",
    "\n",
    "plt.plot(env.check_r)\n",
    "\n",
    "print(env.border_ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.state)\n",
    "#plt.plot([env.KPLX, env.state[2], env.state[0], -env.state[0]], [env.KPLY, env.state[3], env.state[1], env.state[1]])\n",
    "print([env.KPLX, env.state[2], env.state[0], -env.state[0]], [env.KPLY, env.state[3], env.state[1], env.state[1]])\n",
    "env.step(2)[0]\n",
    "#plt.plot([env.KPLX, env.state[2], env.state[0], -env.state[0]], [env.KPLY, env.state[3], env.state[1], env.state[1]])\n",
    "print([env.KPLX, env.state[2], env.state[0], -env.state[0]], [env.KPLY, env.state[3], env.state[1], env.state[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "#plt.plot(env.b0_array + env.KPLX, env.b1_array + env.KPLY)\n",
    "#plt.plot(env.b2_array + env.KPLX, env.b3_array + env.KPLY)\n",
    "#plt.plot(env.b4_array + env.KPLX, env.b5_array)\n",
    "#plt.plot(env.b6_array + env.KPLX, env.b7_array)\n",
    "\n",
    "#plt.plot([env.KPLX, env.Ax0, env.Dx, -env.Dx], [env.KPLY, env.Ay0, env.Dy, env.Dy])\n",
    "#plt.plot(env.z_array, env.b1_array/np.pi*180)\n",
    "#plt.plot(np.flip(env.z_array, 0), env.r_array/2/np.pi*180)\n",
    "plt.plot(reward)\n",
    "#plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.x_array, env.r_array/np.pi*180)\n",
    "plt.plot(env.x_array, env.l_array/np.pi*180)\n",
    "plt.plot(env.x_array, env.k_array/np.pi*180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.k_array[0]/np.pi*180)\n",
    "print(env.r_array[0]/np.pi*180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.k_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.r_array, (env.l_array - env.r_array))\n",
    "plt.plot(env.r_array, (env.k_array - env.r_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.r_array, ((env.k_array - env.l_array)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.tr_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "o = np.empty([])\n",
    "oo = np.empty([])\n",
    "\n",
    "i = 1\n",
    "for i in range(100):\n",
    "    env.step(-100 + 2*i)\n",
    "    o = np.append(o, env.error)\n",
    "    oo = np.append(oo, env.error_orig)\n",
    "    \n",
    "    plt.plot(env.r_array/np.pi*180, env.error_array/np.pi*180)\n",
    "    plt.axvline(x=env.border_ang/np.pi*180)\n",
    "    i += 1\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(o[1:], '.')\n",
    "plt.plot(oo[1:], '.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.border_ang/np.pi*180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.error_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.r_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(env.error_array, env.r_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(env.r_array[0:len(env.error_array)-env.int_chk], o[1:len(env.error_array)-env.int_chk+1], '.')\n",
    "plt.plot(env.r_array, oo[1:], '.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.int_chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.r_array/np.pi*180, env.error_array/np.pi*180)\n",
    "plt.axvline(x=env.border_ang/np.pi*180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "  'a': lambda x: env.Dx = env.Dx + x,\n",
    "  'b': lambda x: x + 7,\n",
    "}['a'](5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import numpy as np\n",
    "x = np.array([0.5156312644066778, 0.526040469557302])\n",
    "y = np.array([0.007969681162248856, 0.0154360504511115])\n",
    "\n",
    "f = interp1d(x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(0.5161168102504706)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
